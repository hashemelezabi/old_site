---
layout: single
title: "CS 221 Lecture 1: Intro to Optimization"
permalink: /posts/cs221-l1/
categories: course-notes
mathjax: true
classes: wide

---

*This blog post is a course notes post: lecture notes written in the style of a blog post. It follows the lecture closely, occasionally taking words verbatim from the slides. Sometimes, I will add extra material beyond the lecture (under "Interlude" or "Extra" subheadings) or implement an algorithm discussed in lecture (making sure not to implement anything assigned as homework). The purposes of these notes are 1) to help me understand the material more deeply by forcing me to explain every detail, 2) to explore the course material beyond lectures, 3) to practice my technical writing, and 4) to provide another source on the material that potentially helps other students.*
{: .notice}

*CS 221: AI Principles and Techniques is an introductory class on AI, covering various techniques and algorithms without going too deeply into any one topic. The description of the course is [here][description] and the course website is [here][website]. This post covers the [first lecture][slides].*
{: .notice--info}

The lecture discusses the huge impact AI has had and gives a very brief history of the field. We then talk briefly about optimization, mentioning two types:

* **Discrete optimization**: optimizing over discrete objects.
    * Examples:
        * Finding the shortest path in a graph.
        * Finding the edit distance between two strings by finding the best sequence of insertions, deletions, and substitutions.
    * Algorithmic tool: **dynamic programming**.
* **Continuous optimization**: optimizing over vectors of real numbers (weights).
    * Examples:
        * Finding the line that best fits a set of data points by minimizing the sum of squared distances (here our weight is a scalar equal to the slope of the best-fit line).
    * Algorithmic tool: **gradient descent**.

This first lecture was mostly nontechnical, with the technical part on optimization summarized above taking up the last part of the class. During the lecture we also discuss the important ethical issues arising around AI systems and the importance of addressing issues of bias and fairness.

The course stands on a nice three-part paradigm:
* **Modeling**: modeling a real-world scenario mathematically, e.g. a graph representing streets in a city, or a state machine representing the operation of a vending machine.
  Within modeling, the course divides modeling techniques into four types, progressing from low-level intelligence (reflex models) to high-level intelligence (logic models):
    * *Reflex-based models*, e.g. machine learning models. Inference on these models is based on a 'reflex' of sorts, where the model takes the input and outputs a prediction without any form of processing or 'thinking'. As an example, when I look at a photo of a zebra I determine that it's a zebra immediately, without thinking about it.
    * *State-based models*, e.g. a state machine where each state represents a state of the game board in a game. An example of inference here would be to choose the best move, which would involve exploring the different states that result from different choices.
    * *Variable-based models*, e.g. constraint satisfaction problems. An example would be a model of a Sudoku game where we try to assign numbers (these are our variables) to squares such that the Sudoku constraints are satisfied. In variable-based models there is no progression or meaningful order like there is in state-based models; we can fill the Sudoku squares in any order we like, for example.
* **Inference**: getting useful information from the model, e.g. asking what the shortest path to a location is, or asking what a given photo depicts.
* **Learning**: enhancing our model using data, e.g. finding a better face detection model by training it on more faces, or finding more descriptive weights on a shortest-path graph using data on congestion and other real-world factors affecting streets.