---
layout: single
title: "CS 166 Lecture 1: Range Minimum Queries, Part 1"
permalink: /posts/cs166-l1/
categories: course-notes
mathjax: true
classes: wide

---

*This blog post is a course notes post. For the most part, it comprises normal lecture notes, like those a scribe would write in a class, written in the style of a blog post. It follows the lecture closely, occasionally taking words verbatim from the slides. Sometimes, I will add extra material beyond the lecture (under "Interlude" or "Extra" subheadings) or implement an algorithm discussed in lecture (making sure not to implement anything assigned as homework). The purposes of these notes are 1) to help me understand the material more deeply by forcing me to explain every detail, 2) to explore the course material beyond lectures, 3) to practice my technical writing, and 4) to provide another source on the material that potentially helps other students.*
{: .notice}

*CS 166: Data Structures, created and taught by Keith Schwarz, delves deep into data structures and teaches powerful associated concepts and techniques. It gives a taste of this exciting area of theoretical computer science, and goes into a lot of advanced material. The description of the course is [here][description] and the course website is [here][website]. This post covers the first lecture, Range Minimum Queries (Part 1), for which the slides can be found [here][slides].*
{: .notice--info}

This lecture introduces the Range Minimum Query problem, abbreviated RMQ. This turns out to be a great way to start off the course since the RMQ problem involves a number of interesting (and beautiful!) concepts and techniques. The statement of the problem, taken from the lecture slides, is the following: given an array $A$ and two indices $i \leq j$, what is the smallest element out of $A[i], A[i+1], \dots, A[j-1], A[j]$? The image below, from the slides, shows an example query for which $26$ is the answer. We will denote an RMQ on $A$ from $i$ to $j$, inclusive, by $RMQ_A(i,j)$. Note that we will use $0$-indexing when we talk about array indices.

![RMQ example](/assets/images/cs166-l1-1.png)

Of course, the trivial solution is to iterate across the given range and take the minimum. But if we are given a fixed array $A$ and we expect many different queries on it - a typical situation in real-world settings - this solution would be very slow since each query would take $O(n)$ time.

In these situations, where we have a fixed piece of data and we want to support many queries on it efficiently, it's natural to consider some form of preprocessing. We can start by considering the simplest form of preprocessing: just precompute all possible RMQs! How many are there? Given an array $A$ of length $n$, there are

* $n$ possible queries of length $1$ ($RMQ_A(i,i)\\ \forall i \in \\{1, \dots, n\\}$),
* $n-1$ possible queries of length $2$ ($RMQ_A(i,i+1)\\ \forall i \in \\{1, \dots, n - 1\\}$),

  $\vdots$

* $2$ possible queries of length $n-1$ ($RMQ_A(0,n-1)$ and $RMQ_A(1,n)$), and
* $1$ possible query of length $n$ ($RMQ_A(0,n)$).

The following screenshot, from the slides, demonstrates this visually for an array with length $5$.

![subarrays](/assets/images/cs166-l1-2.png)

So in total, we have $1 + 2 + \dots + (n - 1) + n = \frac{n}{2}(n+1) = \Theta(n^2)$ possible RMQs. If we precompute all of them, we can answer any query in $O(1)$ time. The preprocessing cost, however, is clearly too expensive. Computing each range takes $O(n)$ time, and since we have $\Theta(n^2)$ ranges, the preprocessing time complexity is $O(n^3)$. A simple argument, based on examining a subset of ranges that are long enough, shows that it is also $\Theta(n^3)$.

With a little more thought, though, we can see that we can compute all possible ranges in time $\Theta(n^2)$ using dynamic programming. To see this, assume we've computed the minimum for all ranges of length $k$. Now take any range of length $k+1$, with start index $i$ and end index $j$. We can get its minimum, $RMQ_A(i,j)$, by computing $min(RMQ_A(i,j-1), RMQ_A(i+1,j))$. So we can proceed in this way, incrementally increasing the length of the ranges to be considered, until we've computed the answer for all possible RMQs. The following screenshot shows one step of this process. Each cell in the table corresponds to a possible range query, with the row labels representing $i$ values and the column labels representing $j$ values. Here, the dynamic programming process just described means we do our precomputations one diagonal at a time until we complete the table. Clearly, this does one piece of computation per cell, so this approach takes $\Theta(n^2)$ time.

![dynamic programming](/assets/images/cs166-l1-3.png)

Following the lecture slides, we will say that an RMQ data structure has time complexity $\langle p(n), q(n) \rangle$ if preprocessing takes time at most $p(n)$ and queries take time at most $q(n)$. We now have two RMQ data structures:

* $\langle O(1), O(n) \rangle$ with no preprocessing.
* $\langle O(n^2), O(1) \rangle$ with full preprocessing.

Our goal in the rest of this post is to explore structures that strike a compromise between these two extremes.

## Block Decomposition

One idea is to split the input array $A$ into $O(n/b)$ blocks of some block size $b$ each, and compute the minimum value in each block. This way, we can compute the minimum of the *block minima*, and will only need to process the individual elements in $A$ that are at the left and right ends of the given range query in the (likely) case that its length is not a multiple of $b$. I remember that Keith gave the analogy of taking the (fast) highway to cover large distances between your origin and destination, and only taking the (slow) small streets when you just left home on your trip or you're about to arrive to your destination. The highway represents the array of block minima, while the small streets at the beginning and end represent the elements to check in the $i$ and $j$ blocks, respectively. The screenshot below, from the slides, shows an example with $b = 3$.

![blocks](/assets/images/cs166-l1-4.png)

We now have another variable $b$ in addition to $n$. For preprocessing, we do $O(b)$ work for each of the $O(n/b)$ blocks to find minima, so $p(n) = O(n)$. For queries, we do $O(b)$ work to scan inside $i$ and $j$'s blocks (the 'left over' entries) and $O(n/b)$ work looking at block minima between $i$ and $j$, so $q(n) = O(b + n/b)$. (Recall that $p(n)$ and $q(n)$ represent preprocessing time complexity and query time complexity respectively.)

We can see a tradeoff here. As $b$ increases, we have more elements to scan within each block and fewer blocks to look at. As $b$ decreases, we have the reverse effect. Is there an optimal choice for $b$? Well, we can choose a $b$ that minimizes the expression $b + n/b$. The derivative of $b + n/b$ with respect to $b$ is $\frac{d}{db} (b+n/b) = 1 - \frac{n}{b^2}$. Setting this to $0$, we get $b^2 = n$, or $b = \sqrt{n}$. This is one of those cases where we use continuous math in an entirely discrete context - pretty cool!

This choice of $b$ yields the asymptotically optimal query runtime, which is $O(b + n/b) = O(n^{1/2} + n / n^{1/2}) = O(n^{1/2} + n^{1/2}) = O(n^{1/2}) = O(\sqrt{n})$.

**Note:** Specifying *asymptotic* optimality serves more than mathematical precision: it is also more accurate in a practical sense. If we only say optimal, the statement might not exactly match reality, in which this choice of $b$ might not be optimal for every possible size of $A$ or every possible query. For example, there are scenarios in which it might be best to have one block ($b = 1$) because jumping to the block minima array, which is a separate array from $A$ that is allocated elsewhere in memory, results in loss of the cache locality we have when processing contiguous elements of $A$. Specifically, this can happen when the query range is too small or $A$ itself is too small.
{: .notice--info}

Now we have a $\langle O(n), O(\sqrt{n}) \rangle$ solution! This lies between our two original extremes. Let's see if we can do better.

## Sparse Tables

Let's revisit the idea of precomputing all possible ranges. The key question here is: do we really need to precompute all possible ranges, which takes time $\Theta(n^2)$, in order to support $O(1)$ queries? On second thought, it seemed that computing all ranges is kind of overkill. In the dynamic programming algorithm we described earlier, to compute $RMQ_A(i,j)$, we take the minimum of $RMQ_A(i,j-1)$ and $RMQ_A(i+1,j)$. Let $k = j - i + 1$. One might notice that the overlap between the two $(k-1)$-length ranges computed for us is too large: it seems that we have more information than we need. Indeed, we could always compute the desired minimum as long as we have the RMQ answer for two smaller subarrays that together cover the range in question.

This is just a loose intuition, but it turns out to be correct: we don't need to precompute all $\Theta(n^2)$ possible ranges. After examining the possible ranges more closely via the table shown in an earlier figure (the roughly half-full matrix), we can find that it is enough to compute the following ranges. For each index $i$, we compute RMQ for ranges starting at $i$ of size $1, 2, 4, 8, \dots, 2^k$ as long as they fit in the array. Thus, we compute $O(\log n)$ ranges for each element, so there are $O(n \log n)$ ranges total to be computed. Any range in the array can be formed as the union of two of these ranges! To answer $RMQ_A(i,j)$, we first find the largest $k$ such that $2^k \leq j - i + 1$. Then the range $[i,j]$ can be formed as the overlap of the ranges $[i,i+2^k-1]$ and $[j - 2^k + 1, j]$, which are both already computed since their size is a power of $2$. It's easy to see that the overlap of these two ranges really forms $[i,j]$. If it didn't, then the size of the range in question ($j - i + 1$) is larger than $2(2^k) = 2^{k+1}$, contradicting that $k$ is the largest integer such that $2^k \leq j - i + 1$.

**Note:** Finding the largest $k$ such that $2^k \leq j - i + 1$ for a given range $[i,j]$ can be done in time $O(1)$ with the right preprocessing. Since this is delegated to Problem Set 1, I can in the same spirit leave this as an exercise to the reader. :)
{: .notice--info}

This way, each range query can be looked up in time $O(1)$. As for precomputing the $O(n \log n)$ ranges, they can all be computed in time $O(n \log n)$ using a dynamic programming strategy similar to what we did earlier. This structure, the *sparse table*, gives an $\langle O(n \log n), O(1) \rangle$ solution to RMQ. This is a great improvement over the "Precompute all" solution!

We now have a bunch of different options:

* Precompute all: $\langle O(n^2), O(1) \rangle$.
* Sparse table: $\langle O(n \log n), O(1) \rangle$.
* Blocking: $\langle O(n), O(\sqrt{n}) \rangle$.
* Precompute none: $\langle O(1), O(n) \rangle$.

In what comes next, we combine strategies we've developed to create hybrid structures that yield even better runtimes.

## Hybrid Structure 1

Consider the blocking structure again. Here's an interesting observation: the problem of finding the minimum across the block minima, and the problem of finding the minimum within $i$ and $j$'s blocks, are just other instances of RMQ! Now we have a new possible route for solving RMQ:

1. Split the input into blocks of some block size $b$.
2. Compute the minimum for each of the $O(n/b)$ blocks.
3. Construct an RMQ structure on the block minima.
4. Construct an RMQ structure on each block.
5. Combine the local RMQ answers to solve RMQ globally.

This decomposition provides a framework through which we can build various data structures combining different choices of 1) the block size $b$, 2) the RMQ structure on top, and 3) the RMQ structure on the blocks below. Let's introduce a formal analysis method for this framework. Suppose the RMQ solution for the block minima has time complexity $\langle p_1(n), q_1(n) \rangle$ and the RMQ solution within each block has time complexity $\langle p_2(n), q_2(n) \rangle$. Then in the hybrid structure, the preprocessing time is $O(n + p_1(n/b) + (n/b) p_2(b))$ and the query time is $O(q_1(n/b) + q_2(b))$.

Notice that the original block-based structure with complexity $\langle O(n), O(\sqrt{n}) \rangle$ uses this framework with the $\langle O(1), O(n) \rangle$ no-preprocessing RMQ structure at the top and bottom. We can use this as a sanity check on the validity of our framework by plugging in values for $p_i(n)$ and $q_i(n)$ $(i = 1,2)$ and confirming that we get the correct time complexity.

Recall that the sparse table takes time $O(n \log n)$ to construct. Then constructing it over the block minima takes time $O((n/b) \log (n/b))$. We have $\log (n/b) = \log(n) - \log(b) = O(\log n)$ ($b < n$), the time to build the sparse table is at most $O((n/b) \log n)$. Here's a cute trick: if we take $b = \Theta(\log n)$, the time becomes $O((n/b) \log n) = O((n / \log n) \log n) = O(n)$.

With this, we can build our first hybrid: set the block size to $\log n$, use a sparse table for the top-level structure, and use the "no preprocessing" structure for each block. This yields a time complexity of $\langle O(n), O(\log n) \rangle$! This improves on the original $\langle O(n), O(\sqrt{n}) \rangle$ blocking structure.

## Interlude: Segment Trees

Before going further in the lecture, I mention another structure that has time complexity $\langle O(n), O(\log n) \rangle$. I learned about this by chance in a [GeeksforGeeks post][geeks] when looking up RMQ, and it was fun to find yet another structure that provides a pretty good runtime. That structure is the segment tree, and it's quite simple. Each leaf node in the segment tree is an element of the input array, while each internal node holds the minimum element among all elements (leaves) under it. So each internal node represents an interval, or segment, of the array, and holds the minimum over that segment.

I try to give a comprehensive explanation of how to build and use the segment tree here, but I refer the reader to the [GeeksforGeeks post][geeks] for more details.

### Preprocessing

The segment tree is constructed recursively as follows. We start with the full array and keep dividing the current array into two parts until we have two $1$-element arrays. Now we take the smaller value and set it to the parent node, and keep building upward towards the root and computing the minima along the way by backtracking through the recursive calls.

Since we divide the array into two parts each time, there will be $O(\log n)$ levels and $O(2^{\log n}) = O(n)$ nodes. Notice that only the last level in the segment tree might not be completely filled, in the case when an internal node represents a range of length $3$ which is then divided into a length-$2$ range node $n_1$ and a length-$1$ range leaf node $n_2$. $n_1$ will have two leaves as children, while $n_2$ won't have any children, making the last level incomplete. This incompleteness occurs only at the last level since it occurs precisely when the internal node represents a length-$3$ range. This explains why the number of levels is $O(\log n)$ and, consequently, the number of nodes is $O(n)$.

Since we do one computation per node, the preprocessing time is $O(n)$ this way.

### Query

The query procedure is recursive. Given a query range, we start at the root, setting it to be the current node. We check if the current node's range is contained in or equal to the query range. We proceed as follows:

* If it is, we return the minimum value stored in the current node.
* Else if the node's range and the query range are disjoint, we return an 'infinite' value (eg. `float('inf')` for Python or `INT_MAX` for C++).
* Else, we recursively call our function on the left and right children, giving it the same query range, and return the minimum of them.

Pseudocode (as well as implementation) of this can be found in the [GeeksforGeeks post][geeks]. The correctness of this algorithm 

time complexity O(log n)

This has the same preprocessing and query time complexities as our first hybrid structure above. The two structures are quite different though, and it's likely that one will beat the other in practice due to real-world factors. In the following Jupyter notebook, built using the [xeus-cling][xeus-cling] C++ kernel and embedded here in static form, I implemented the segment tree and compared its performance with the Hybrid 1 structure described above. I test two different implementations of the segment tree, one using a node pointer model and one using an array model. Since the Hybrid 1 structure implementation is part of [Problem Set 1][pset1], I only use its binary executable in the notebook.

hypothesis

{% include cs166-l1-notebook.html %}

## Hybrid Structure 2

We can try another thing: use the sparse table for both the top and bottom RMQ structures, with a block size of $\log n$.

Recall that in the hybrid structure, the preprocessing time is $O(n + p_1(n/b) + (n/b) p_2(b))$ and the query time is $O(q_1(n/b) + q_2(b))$. Plugging in, we get a $\langle O(n \log \log n), O(1) \rangle$ solution to RMQ! This hybrid improves on the sparse table by adding a repeated logarithm instead of one logarithm. In Problem Three in [Problem Set 1][pset1] we show that we can build RMQ structures with arbitrarily many repeated logarithms using this hybrid framework.

## Hybrid Structure 3

For our last hybrid, suppose we get a little creative and use a sparse table for the top structure and the $\langle O(n), O(\log n) \rangle$ Hybrid 1 solution for our bottom structure!

Plugging in, this gives a $\langle O(n), O(\log \log n) \rangle$ solution -- a repeated-logarithm improvement over Hybrid 1!

## Where We Stand

Omitting the no-preprocessing option, here are the RMQ structures we've covered:

* Full preprocessing: $\langle O(n^2), O(1) \rangle$.
* Sparse table: $\langle O(n \log n), O(1) \rangle$.
* Hybrid 2: $\langle O(n \log \log n), O(1) \rangle$.

* Blocking: $\langle O(n), O(\sqrt{n}) \rangle$.
* Hybrid 1: $\langle O(n), O(\log n) \rangle$.
* Hybrid 3: $\langle O(n), O(\log \log n) \rangle$.

Notice that the first three offer $O(1)$ query time, while the last three offer $O(n)$ preprocessing time. Amazingly, there is a golden $\langle O(n), O(1) \rangle$ RMQ data structure! It uses beautiful ideas, and it's the topic of the next lecture.


[description]: https://explorecourses.stanford.edu/search?view=catalog&filter-coursestatus-Active=on&page=0&catalog=&academicYear=&q=cs166&collapse=
[website]: http://web.stanford.edu/class/cs166/
[slides]: http://web.stanford.edu/class/cs166/lectures/00/Slides00.pdf
[geeks]: https://www.geeksforgeeks.org/segment-tree-set-1-range-minimum-query/
[xeus-cling]: https://github.com/QuantStack/xeus-cling
[pset1]: http://web.stanford.edu/class/cs166/handouts/060%20Problem%20Set%201.pdf
