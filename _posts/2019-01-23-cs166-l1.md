---
layout: single
title: "CS 166 Lecture 1: Range Minimum Queries, Part 1"
permalink: /posts/cs166-l1/
categories: course-notes
mathjax: true
classes: wide

---

*CS 166: Data Structures, created and taught by Keith Schwarz, delves deep into data structures and teaches powerful associated concepts and techniques. It gives a taste of this exciting area of theoretical computer science, and goes into a lot of advanced material. The description of the course is [here][description] and the course website is [here][website]. This post covers the first lecture, Range Minimum Queries (Part 1), for which the slides can be found [here][slides].*

This lecture introduces the Range Minimum Query problem, abbreviated RMQ. This turns out to be a great way to start off the course since the RMQ problem involves a number of interesting (and beautiful!) concepts and techniques. The statement of the problem, taken from the lecture slides, is the following: given an array $A$ and two indices $i \leq j$, what is the smallest element out of $A[i], A[i+1], \dots, A[j-1], A[j]$? The image below, from the slides, shows an example query for which $26$ is the answer. We will denote an RMQ on $A$ from $i$ to $j$, inclusive, by $RMQ_A(i,j)$. Note that we will use $0$-indexing when we talk about array indices.

![RMQ example](/assets/images/cs166-l1-1.png)

Of course, the trivial solution is to iterate across the given range and take the minimum. But if we are given a fixed array $A$ and we expect many different queries on it - a typical situation in real-world settings - this solution would be very slow since each query would take $O(n)$ time.

In these situations, where we have a fixed piece of data and we want to support many queries on it efficiently, it's natural to consider some form of preprocessing. We can start by considering the simplest form of preprocessing: just precompute all possible RMQs! How many are there? Given an array $A$ of length $n$, there are

* $n$ possible queries of length $1$ ($RMQ_A(i,i)\\ \forall i \in \\{1, \dots, n\\}$),
* $n-1$ possible queries of length $2$ ($RMQ_A(i,i+1)\\ \forall i \in \\{1, \dots, n - 1\\}$),

  $\vdots$

* $2$ possible queries of length $n-1$ ($RMQ_A(0,n-1)$ and $RMQ_A(1,n)$), and
* $1$ possible query of length $n$ ($RMQ_A(0,n)$).

The following screenshot, from the slides, demonstrates this visually for an array with length $5$.

![subarrays](/assets/images/cs166-l1-2.png)

So in total, we have $1 + 2 + \dots + (n - 1) + n = \frac{n}{2}(n+1) = \Theta(n^2)$ possible RMQs. If we precompute all of them, we can answer any query in $O(1)$ time. The preprocessing cost, however, is clearly too expensive. Computing each range takes $O(n)$ time, and since we have $\Theta(n^2)$ ranges, the preprocessing time complexity is $O(n^3)$. A simple argument, based on examining a subset of ranges that are long enough, shows that it is also $\Theta(n^3)$.

With a little more thought, though, we can see that we can compute all possible ranges in time $\Theta(n^2)$ using dynamic programming. To see this, assume we've computed the minimum for all ranges of length $k$. Now take any range of length $k+1$, with start index $i$ and end index $j$. We can get its minimum, $RMQ_A(i,j)$, by computing $min(RMQ_A(i,j-1), RMQ_A(i+1,j))$. So we can proceed in this way, incrementally increasing the length of the ranges to be considered, until we've computed the answer for all possible RMQs. The following screenshot shows one step of this process. Each cell in the table corresponds to a possible range query, with the row labels representing $i$ values and the column labels representing $j$ values. Here, the dynamic programming process just described means we do our precomputations one diagonal at a time until we complete the table. Clearly, this does one piece of computation per cell, so this approach takes $\Theta(n^2)$ time.

![dynamic programming](/assets/images/cs166-l1-3.png)

Following the lecture slides, we will say that an RMQ data structure has time complexity $\langle p(n), q(n) \rangle$ if preprocessing takes time at most $p(n)$ and queries take time at most $q(n)$. We can have two RMQ data structures:

* $\langle O(1), O(n) \rangle$ with no preprocessing.
* $\langle O(n^2), O(1) \rangle$ with full preprocessing.

Our goal in the rest of this post is to explore structures that strike a compromise between these two extremes.

## Block Decomposition

The idea here is to split the input array $A$ into $O(n/b)$ blocks of some block size $b$ each, and compute the minimum value in each block. This way, we can compute the minimum of the *block minima*, and will only need to process the individual elements in $A$ that are at the left and right ends of the given range query in the (likely) case that its length is not a multiple of $b$. The screenshot below, from the slides, shows an example with $b = 3$.

![blocks](/assets/images/cs166-l1-4.png)

We now have another variable $b$ in addition to $n$. For preprocessing, we do $O(b)$ work for each of the $O(n/b)$ blocks to find minima, so $p(n) = O(n)$. For queries, we do $O(b)$ work to scan inside $i$ and $j$'s blocks (the 'left over' entries) and $O(n/b)$ work looking at block minima between $i$ and $j$, so $q(n) = O(b + n/b)$. (Recall that $p(n)$ and $q(n)$ represent preprocessing time complexity and query time complexity respectively.)

We can see a tradeoff here. As $b$ increases, we have more elements to scsan within each block and fewer blocks to look at. As $b$ decreases, we have the reverse effect. Is there an optimal choice for $b$? Well, we can choose a $b$ that minimizes the expression $b + n/b$. The derivative of $b + n/b$ with respect to $b$ is $\frac{d}{db} (b+n/b) = 1 - \frac{n}{b^2}$. Setting this to $0$, we get $b^2 = n$, or $b = \sqrt{n}$. This is one of those cases where we use continuous math in an entirely discrete context - pretty cool!

This choice of $b$ yields the asymptotically optimal query runtime, which is $O(b + n/b) = O(n^{1/2} + n / n^{1/2}) = O(n^{1/2} + n^{1/2}) = O(n^{1/2}) = O(\sqrt{n}).

**Note:** We specify *asymptotic* optimality because this yields the slowest growth for a mathematical function. In reality, it might not be optimal for every possible size of $A$ or every possible query. For example, there are scenarios in which it might be best to have one block ($b = 1$) because jumping to the block minima array, which is a separate array from $A$ that is allocated elsewhere in memory, results in loss of the cache locality we have when processing contiguous elements of $A$. Specifically, this can happen when the query range is too small or $A$ itself is too small.
{: .notice--info}

This adds 


[description]: https://explorecourses.stanford.edu/search?view=catalog&filter-coursestatus-Active=on&page=0&catalog=&academicYear=&q=cs166&collapse=
[website]: http://web.stanford.edu/class/cs166/
[slides]: http://web.stanford.edu/class/cs166/lectures/00/Slides00.pdf
